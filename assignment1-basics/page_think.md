1. 为什么是随机取start，而不是按顺序?
A：
在语言模型训练中，我们通常采用随机抽样的方式来获取批次（batch）数据，这样做主要有以下几个好处：

防止模型过拟合：如果总是按顺序处理数据，模型可能会记住训练数据的特定模式或顺序，而不是学习通用的语言规律。随机取样可以确保模型在每次迭代中都看到不同组合的数据，从而提高其泛化能力，使其在未见过的数据上表现更好。

提高训练效率：对于大型数据集，按顺序训练意味着要遍历整个数据集才能完成一个“epoch”。随机抽样可以让我们在每个epoch中都使用不同的数据子集，避免冗余，并且能更快地让模型收敛到一个较好的状态。

避免数据偏差：数据集可能存在某种特定的顺序，例如按时间排序或者按主题排序。如果按顺序训练，模型在开始时可能只学习到一种类型的数据特征，而对其他类型的数据学习不足。随机抽样可以打乱这种顺序，确保每个批次都能代表整个数据集的分布。


2. 一个比较乱，个人一直理解不清的问题，怎么算loss（或者说loss怎么体现对一个句子每个token都起作用）
batch维度是批次大小，一个批次用batch个样本
sequence维度体现一个句子有多个token
d_model维度体现一个token的隐藏状态
（比如我们训练一次取一个batch，有4个样本（4个sequence），每个长度是8个token）

我们在sequence维度就是把这个句子所有token的loss计算了
例如x：[[32,58,67]] y: [[58,67,93]]
那么x最后输出[[[vocab_size], [vocab_size], [vocab_size]]]
cross_entropy, x view成[[vocab_size], [vocab_size], [vocab_size]]，
y view成[58,67,93]。分别取58、67、93（即下一个token的位置）计算softmax。再取-log softmax，即是一个位置的loss（比如58），你需要对sequence所有位置均计算，比如58、67、93，求和，再对batch维度（即所有句子）也计算求和。即最后是对一个batch内所有句子的所有token都计算loss，即取下一个token位置的softmax，然后求和取平均，就是loss，所以一个句子所有token都得到了训练


3. 以训练llm举例，哪些放在cpu，哪些放在gpu：
1. 資料準備階段（在 CPU 上）
這個階段主要由 CPU 負責，因為它涉及大量的檔案讀寫和資料處理。

資料讀取和預處理： 從硬碟中讀取原始文本資料集（例如，Common Crawl、維基百科）。這包括清理文本、分詞（tokenization）和將文字轉換成模型可以理解的數字序列。

資料載入和批次化（Batching）： CPU 會將預處理好的資料整理成一個個的資料批次（batch），這些批次會被準備好，以便隨後傳輸給 GPU 進行模型訓練。

2. 模型訓練階段（在 GPU 上）
這是 GPU 核心發揮作用的階段，所有的高效能計算都在這裡進行。

模型參數和資料批次： 在每個訓練步驟開始時，CPU 會將準備好的資料批次和模型參數一起傳輸到 GPU 的顯存中。

前向傳播（Forward Pass）： 資料在 GPU 上通過 LLM 的所有層，進行大量的矩陣乘法和其他運算，最終生成模型的預測結果。

反向傳播（Backward Pass）和參數更新： 根據預測結果與真實標籤的差異，GPU 會計算梯度（gradients），然後用梯度來更新模型的數百億甚至數千億個參數。這個過程是訓練 LLM 中計算量最大、最耗時的部分。

3. 模型儲存和評估（在 CPU 上）
當訓練完成或達到某個檢查點時，模型會被儲存，這個過程通常由 CPU 負責。

儲存檢查點（Checkpointing）： 為了防止訓練中斷，我們會定期將模型的當前參數從 GPU 顯存傳回到 CPU，並儲存在硬碟上。

模型評估和微調： 訓練後的模型在 CPU 上加載，並在測試集上進行評估。如果需要進行微調或推理，參數會再次根據需求被傳輸到 GPU 上。

CPU（中央处理器）
CPU 被设计用于串行处理。它通常有几个强大的核心，每个核心都擅长以最快的速度执行复杂的、顺序性的指令。例如，处理硬盘读写、逻辑判断、或者像文本编码这种需要大量步骤但无法高度并行化的任务。CPU 就像一个高技能的专家，能快速高效地完成一项项复杂的任务。

GPU（图形处理器）
GPU 则被设计用于并行处理。它拥有数以千计的小型核心，虽然单个核心的计算能力远不如 CPU，但它们可以同时执行相同的简单计算任务。这使得 GPU 在处理像矩阵乘法（线性代数）这样的任务时效率极高，因为这些任务可以分解成成千上万个可以同时进行的微小计算。GPU 就像一个拥有大量工人的流水线工厂，非常适合重复性的、可并行化的工作。

在深度学习中，训练过程中的核心计算（前向传播和反向传播）本质上就是大规模的矩阵运算。将这些计算放到 GPU 上，就像把工厂的流水线开足马力，可以显著加快训练速度。而像数据加载、文件读写这些无法并行化的任务，则由 CPU 负责，这样就实现了最高效的分工。

4. top_p vs top_k
Top-p (核采样)
top_p 采样的作用是动态地选择一个词汇子集，这个子集中的词的总概率之和达到设定的阈值 p。

工作原理：模型会首先对所有可能的下一个词的概率进行降序排序，然后从最高概率的词开始累加，直到累积概率达到 p 的值。然后，模型只从这个累积概率集合中的词里进行采样，将集合外的所有词的概率都设为零。

优点：与 top_k 相比，top_p 能更好地适应不同的文本上下文。在概率分布平缓（可能有很多同样合适的词）时，它会考虑更多的词；在概率分布尖锐（只有一个词明显更合适）时，它会只考虑少数几个词。这使得生成结果既不过于保守，也不过于随机。

Top-k
top_k 采样的作用是限制模型只从概率最高的 k 个词中进行选择。

工作原理：模型会首先识别出概率最高的 k 个词，然后将其他所有词的概率都设为零。最后，模型会从这 k 个词中进行随机采样。

优点：这是一种简单有效的控制生成随机性的方法。通过调整 k 的值，你可以直接控制下一个词的选择范围。

缺点：k 值是固定的，这可能在某些情况下不够灵活。例如，如果 k 设定为 50，但在某个上下文中只有 10 个词是合理的，那么这 50 个词中剩下的 40 个可能是不合适的。反之，如果在一个有 1000 个合理候选项的上下文中使用 top_k=50，那么你可能会错过很多好的词。
(就是top_p如果高概率的比较多，那top_p都可以选到；高概率比较少，那top_p也不会多选，只会选到比较多的几个)

5. 为什么top_k?
保证质量：通过排除所有低概率的词，Top-k 确保了生成的词不会是完全不相关的，从而提高了文本的连贯性。
解决“长尾问题”：它可以有效避免从词汇表中那些极低概率的“长尾”部分采样到不合逻辑的词。

6. 温度作用：
温度参数的作用是通过调整模型的输出概率分布来控制生成文本的随机性。

高温（temperature > 1.0）：使概率分布变得更加平缓。即使是概率较低的词，被选中的机会也会增加。这会产生更多样、更具创造性、更随机的文本，但可能会牺牲一定的逻辑连贯性。

低温（temperature < 1.0）：使概率分布变得更加尖锐。模型会更倾向于选择那些概率最高的词。这会使生成的文本更具确定性，更符合模型“认为”最可能出现的词，但可能导致文本重复或缺乏创造性。

温度等于 1.0：这是默认值，不改变原始概率分布。


